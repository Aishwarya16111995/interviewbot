from flask import Flask, render_template, request, jsonify, session
import openai
import os
from datetime import datetime, timezone
import base64
from gtts import gTTS
import tempfile
from pydub import AudioSegment
import cv2
import time
import numpy as np
import re
import json
import logging
from logging.handlers import RotatingFileHandler

app = Flask(__name__)
app.secret_key = 'your-secret-key-here'
app.config['PERMANENT_SESSION_LIFETIME'] = 4600

# Configure logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)
handler = RotatingFileHandler('interview_app.log', maxBytes=10000000, backupCount=5)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)
logger.setLevel(logging.DEBUG)

# OpenAI API Configuration
openai.api_key = "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"
openai.api_base = "https://api.openai.com/v1"

# Configuration
MAX_FRAME_SIZE = 500
FRAME_CAPTURE_INTERVAL = 5
MAX_RECORDING_DURATION = 520
PAUSE_THRESHOLD = 40
FOLLOW_UP_PROBABILITY = 0.8
MIN_FOLLOW_UPS = 1
MAX_FOLLOW_UPS = 2  # Exactly 2 follow-ups per question
CONVERSATION_FILE = "interview_conversation.txt"

def init_interview_data():
    logger.debug("Initializing new interview data structure")
    # Clear previous conversation file
    if os.path.exists(CONVERSATION_FILE):
        os.remove(CONVERSATION_FILE)
    return {
        "questions": [],
        "answers": [],
        "ratings": [],
        "current_question": 0,
        "interview_started": False,
        "conversation_history": [],
        "role": "",
        "experience_level": "",
        "years_experience": 0,
        "start_time": None,
        "end_time": None,
        "visual_feedback": [],
        "last_frame_time": 0,
        "last_activity_time": None,
        "follow_up_questions": [],
        "current_topic": None,
        "follow_up_count": 0,
        "current_context": "",
        "question_topics": [],
        "used_questions": [],
        "used_follow_ups": []
    }

def save_conversation_to_file(conversation_data):
    try:
        with open(CONVERSATION_FILE, "a") as f:
            for item in conversation_data:
                if 'speaker' in item:
                    f.write(f"{item['speaker']}: {item['text']}\n")
                elif 'question' in item:
                    f.write(f"Question: {item['question']}\n")
        logger.debug("Conversation saved to file")
    except Exception as e:
        logger.error(f"Error saving conversation to file: {str(e)}", exc_info=True)

def load_conversation_from_file():
    try:
        if not os.path.exists(CONVERSATION_FILE):
            return []
        
        with open(CONVERSATION_FILE, "r") as f:
            lines = f.readlines()
        
        conversation = []
        for line in lines:
            if line.startswith("bot:") or line.startswith("user:"):
                speaker, text = line.split(":", 1)
                conversation.append({"speaker": speaker.strip(), "text": text.strip()})
            elif line.startswith("Question:"):
                question = line.split(":", 1)[1].strip()
                conversation.append({"question": question})
        
        return conversation
    except Exception as e:
        logger.error(f"Error loading conversation from file: {str(e)}", exc_info=True)
        return []

@app.before_request
def before_request():
    logger.debug(f"Before request - path: {request.path}, method: {request.method}")
    if 'interview_data' not in session:
        logger.debug("No interview data in session, initializing new data")
        session['interview_data'] = init_interview_data()
    session.permanent = True

def generate_initial_questions(role, experience_level, years_experience):
    logger.debug(f"Generating initial questions for role: {role}, experience: {experience_level}, years: {years_experience}")
    
    # Load previous conversation to avoid repetition
    previous_conversation = load_conversation_from_file()
    previous_questions = [item['text'] for item in previous_conversation if 'speaker' in item and item['speaker'] == 'bot']
    
    if experience_level == "fresher":
        prompt = f"""
        Generate a professional interview script for a {role} position (fresher candidate).
        Avoid asking questions that have already been asked in previous interviews:
        {previous_questions}
        
        The interview should follow this structure:
        1. Start with a simple greeting
        2. Ask about the candidate's background and education
        3. Ask exactly 2 technical questions appropriate for a fresher
        4. Ask 1 behavioral question
        5. End with a closing statement
        
        For each main question, generate exactly 2 potential follow-up questions that could naturally follow based on typical answers.
        Format the output as:
        Main Question: [question]
        Follow-ups: [follow-up 1] | [follow-up 2]
        ---
        """
    else:
        prompt = f"""
        Generate a professional interview script for a {role} position (candidate with {years_experience} years experience).
        Avoid asking questions that have already been asked in previous interviews:
        {previous_questions}
        
        The interview should follow this structure:
        1. Start with a simple greeting
        2. Ask about the candidate's experience and background
        3. Ask exactly 2 technical questions appropriate for their experience level
        4. Ask 1 behavioral/situational question
        5. Ask 1 question about their achievements
        6. End with a closing statement
        
        For each main question, generate exactly 2 potential follow-up questions that could naturally follow based on typical answers.
        Format the output as:
        Main Question: [question]
        Follow-ups: [follow-up 1] | [follow-up 2]
        ---
        """
    
    try:
        logger.debug("Sending prompt to OpenAI for question generation")
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=2000
        )
        script = response.choices[0].message.content
        logger.debug("Received response from OpenAI for question generation")
        
        questions = []
        question_topics = []
        current_block = {}
        
        for line in script.split("\n"):
            if line.startswith("Main Question:"):
                if current_block:
                    questions.append(current_block)
                current_block = {
                    "main": line.replace("Main Question:", "").strip(),
                    "follow_ups": []
                }
            elif line.startswith("Follow-ups:"):
                follow_ups = line.replace("Follow-ups:", "").strip().split("|")
                current_block["follow_ups"] = [fq.strip() for fq in follow_ups if fq.strip()][:2]
                question_topics.append(extract_topic(current_block["main"]))
            elif line.strip() == "---" and current_block:
                questions.append(current_block)
                current_block = {}
        
        if current_block:
            questions.append(current_block)
            
        # Ensure we have exactly 2 technical questions
        if experience_level == "fresher":
            questions = questions[:4]  # Greeting, 2 tech, 1 behavioral
        else:
            questions = questions[:5]  # Greeting, 2 tech, 1 behavioral, 1 achievement
            
        logger.debug(f"Generated {len(questions)} questions and {len(question_topics)} topics")
        return questions, question_topics[:len(questions)]
    
    except Exception as e:
        logger.error(f"Error generating questions: {str(e)}", exc_info=True)
        if experience_level == "fresher":
            questions = [
                {
                    "main": "Welcome to the interview. Could you tell us about your educational background?",
                    "follow_ups": [
                        "What specific courses did you find most valuable?",
                        "How has your education prepared you for this role?"
                    ]
                },
                {
                    "main": "What programming languages are you most comfortable with?",
                    "follow_ups": [
                        "Can you describe a project where you used [language]?",
                        "What's your approach to learning new programming languages?"
                    ]
                },
                {
                    "main": "Can you explain a technical concept you learned recently?",
                    "follow_ups": [
                        "How have you applied this concept in practice?",
                        "What challenges did you face while learning this?"
                    ]
                },
                {
                    "main": "Describe a time you faced a challenge in a team project.",
                    "follow_ups": [
                        "What specific role did you play in resolving this?",
                        "What did you learn about teamwork from this experience?"
                    ]
                }
            ]
            question_topics = [extract_topic(q["main"]) for q in questions]
        else:
            questions = [
                {
                    "main": "Welcome to the interview. Could you summarize your professional experience?",
                    "follow_ups": [
                        "What aspect of your experience is most relevant to this role?",
                        "What's been your biggest professional growth area?"
                    ]
                },
                {
                    "main": "What technical challenges have you faced in your recent projects?",
                    "follow_ups": [
                        "Walk me through how you approached solving one of these challenges",
                        "How did you ensure the solution was maintainable?"
                    ]
                },
                {
                    "main": "Tell us about a time you had to lead a project or team.",
                    "follow_ups": [
                        "What leadership style did you adopt and why?",
                        "What metrics did you use to measure the project's success?"
                    ]
                },
                {
                    "main": "Describe your most significant professional achievement.",
                    "follow_ups": [
                        "What specific skills contributed to this achievement?",
                        "What lessons from this achievement do you still apply today?"
                    ]
                }
            ]
            question_topics = [extract_topic(q["main"]) for q in questions]
        logger.warning("Using fallback questions due to API error")
        return questions, question_topics

def extract_topic(question):
    logger.debug(f"Extracting topic from question: {question}")
    question = question.lower()
    if 'tell me about' in question:
        return question.split('tell me about')[-1].strip(' ?')
    elif 'describe' in question:
        return question.split('describe')[-1].strip(' ?')
    elif 'explain' in question:
        return question.split('explain')[-1].strip(' ?')
    elif 'what' in question:
        return question.split('what')[-1].strip(' ?')
    elif 'how' in question:
        return question.split('how')[-1].strip(' ?')
    return question.split('?')[0].strip()

def generate_dynamic_follow_up(conversation_history, current_topic):
    logger.debug(f"Generating dynamic follow-up for topic: {current_topic}")
    try:
        prompt = f"""
        Based on the candidate's last response about '{current_topic}', generate a relevant, insightful follow-up question.
        The question should:
        1. Be directly related to specific details in their response
        2. Probe deeper into their experience, knowledge, or thought process
        3. Be professional and appropriate for a job interview
        4. Be concise (one sentence)
        
        Candidate's last response: "{conversation_history[-1]['text']}"
        
        Return ONLY the question, nothing else.
        """
        
        logger.debug("Sending prompt to OpenAI for dynamic follow-up")
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=200
        )
        follow_up = response.choices[0].message.content.strip()
        logger.debug(f"Generated follow-up question: {follow_up}")
        return follow_up if follow_up.endswith('?') else follow_up + '?'
    except Exception as e:
        logger.error(f"Error generating dynamic follow-up: {str(e)}", exc_info=True)
        return None

def generate_encouragement_prompt(conversation_history):
    logger.debug("Generating encouragement prompt for paused candidate")
    try:
        prompt = f"""
        The candidate has paused during their response. Generate a brief, encouraging prompt to:
        - Help them continue their thought
        - Reference specific aspects of their previous answers
        - Be supportive and professional
        - Be concise (one short sentence)
        
        Current conversation context:
        {conversation_history[-2:]}
        
        Return ONLY the prompt, nothing else.
        """
        
        logger.debug("Sending prompt to OpenAI for encouragement generation")
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5,
            max_tokens=300
        )
        encouragement = response.choices[0].message.content.strip()
        logger.debug(f"Generated encouragement: {encouragement}")
        return encouragement
    except Exception as e:
        logger.error(f"Error generating encouragement prompt: {str(e)}", exc_info=True)
        return "Please continue with your thought."

def text_to_speech(text):
    logger.debug(f"Converting text to speech: {text[:50]}...")
    try:
        tts = gTTS(text=text, lang='en', slow=False)
        with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as temp_file:
            temp_filename = temp_file.name
        
        tts.save(temp_filename)
        
        audio = AudioSegment.from_mp3(temp_filename)
        wav_filename = temp_filename.replace('.mp3', '.wav')
        audio.export(wav_filename, format="wav")
        
        with open(wav_filename, 'rb') as f:
            audio_data = f.read()
        
        os.unlink(temp_filename)
        os.unlink(wav_filename)
        logger.debug("Successfully converted text to speech")
        return base64.b64encode(audio_data).decode('utf-8')
    except Exception as e:
        logger.error(f"Error in text-to-speech: {str(e)}", exc_info=True)
        return None

def process_frame_for_gpt4v(frame):
    logger.debug("Processing frame for GPT-4 Vision")
    height, width = frame.shape[:2]
    if height > MAX_FRAME_SIZE or width > MAX_FRAME_SIZE:
        scale = MAX_FRAME_SIZE / max(height, width)
        frame = cv2.resize(frame, (int(width * scale), int(height * scale)))
    
    _, buffer = cv2.imencode('.jpg', frame)
    frame_base64 = base64.b64encode(buffer).decode('utf-8')
    logger.debug("Frame processed successfully")
    return frame_base64

def analyze_visual_response(frame_base64, conversation_context):
    logger.debug("Analyzing visual response with GPT-4 Vision")
    try:
        response = openai.ChatCompletion.create(
            model="gpt-4-turbo",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": f"Analyze this interview candidate's appearance and environment. "
                                                f"Current conversation context: {conversation_context[-3:] if len(conversation_context) > 3 else conversation_context}"
                                                "Provide brief professional feedback on:" 
                                                "1. Professional appearance (if visible)"
                                                "2. Body language and posture"
                                                "3. Environment appropriateness"
                                                "4. Any visual distractions"
                                                "Keep response under 50 words."},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{frame_base64}"
                            }
                        }
                    ]
                }
            ],
            max_tokens=200
        )
        feedback = response.choices[0].message.content
        logger.debug(f"Visual feedback received: {feedback}")
        return feedback
    except Exception as e:
        logger.error(f"Error in visual analysis: {str(e)}", exc_info=True)
        return None

def evaluate_response(answer, question, role, experience_level, visual_feedback=None):
    logger.debug(f"Evaluating response for question: {question[:50]}...")
    if len(answer.strip()) < 20:
       logger.debug("Answer too short, returning 2")
       return 2
    elif len(answer.strip()) < 50:
       logger.debug("Short but acceptable answer, returning 4")
       return 4

    rating_prompt = f"""
    Analyze this interview response for a {role} position ({experience_level} candidate).
    Question: "{question}"
    Answer: "{answer}"
    
    Provide ONLY a numeric rating from 1-10 based on:
    - Relevance to question (20%)
    - Depth of knowledge (30%)
    - Clarity of communication (20%)
    - Specific examples provided (20%)
    - Professionalism (10%)
    
    Return ONLY the number between 1-10, nothing else.
    """
    
    try:
        messages = [
            {"role": "system", "content": "You are an honest interviewer who provides accurate ratings."},
            {"role": "user", "content": rating_prompt}
        ]
        
        if visual_feedback:
            messages.append({
                "role": "system",
                "content": f"Visual feedback: {visual_feedback}"
            })
        
        logger.debug("Sending evaluation request to OpenAI")
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=messages,
            temperature=0.4,
            max_tokens=100
        )
        rating_text = response.choices[0].message.content
        
        try:
            rating = float(rating_text.strip())
            final_rating = max(1, min(10, rating))
            logger.debug(f"Evaluated response rating: {final_rating}")
            return final_rating
        except:
            logger.warning(f"Could not parse rating: {rating_text}, returning default")
            return 5
    except Exception as e:
        logger.error(f"Error evaluating response: {str(e)}", exc_info=True)
        return 5

def generate_interview_report(interview_data):
    try:
        # Calculate interview duration
        duration = "N/A"
        if interview_data['start_time'] and interview_data['end_time']:
            duration_seconds = (interview_data['end_time'] - interview_data['start_time']).total_seconds()
            minutes = int(duration_seconds // 60)
            seconds = int(duration_seconds % 60)
            duration = f"{minutes}m {seconds}s"
        # Calculate average rating
        avg_rating = sum(interview_data['ratings']) / len(interview_data['ratings']) if interview_data['ratings'] else 0
        logger.debug(f"Average rating calculated: {avg_rating:.1f}")
        
        # Determine status based on average rating
        if avg_rating >= 7:
            status = "Selected"
            status_class = "selected"
        elif avg_rating >= 4 and avg_rating < 7:
            status = "On Hold"
            status_class = "onhold"
        else:
            status = "Rejected"
            status_class = "rejected"
        logger.debug(f"Interview status determined: {status}")
        
        # Prepare conversation history for analysis
        conversation_history_text = "\n".join(
            [f"{item['speaker']}: {item['text']}" for item in interview_data['conversation_history'] if 'speaker' in item]
        )
        
        # Generate comprehensive report using GPT
        report_prompt = f"""
        Analyze this interview transcript and generate a detailed report for a {interview_data['role']} position candidate.
        
        Candidate Background:
        - Experience Level: {interview_data['experience_level']}
        - Years of Experience: {interview_data['years_experience']}
        - Interview Duration: {duration}
        - Average Rating: {avg_rating:.1f}/10
        
        Interview Transcript:
        {conversation_history_text}
        
        Please provide a comprehensive report with the following sections:
        1. Interview Summary (brief overview)
        2. Key Strengths (3 specific strengths with examples from answers)
        3. Areas for Improvement (3 specific areas with actionable suggestions)
        4. Overall Recommendation (Selected/On Hold/Rejected)
        5. Voice Feedback Script (a concise 5-6 line summary in conversational tone)
        
        Format the report in HTML with appropriate headings and styling.
        Include tables for strengths and improvements with two columns (Aspect, Evidence/Suggestion).
        """
        
        logger.debug("Sending report generation request to OpenAI")
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[{"role": "user", "content": report_prompt}],
            temperature=0.5,
            max_tokens=2000
        )
        
        report_content = response.choices[0].message.content
        logger.debug("Received report content from OpenAI")
        
        # Extract voice feedback from the report
        voice_feedback_prompt = f"""
        Extract or create a concise 5-6 line voice feedback summary from this interview report:
        {report_content}
        
        The feedback should:
        - Be spoken in a natural, conversational tone
        - Highlight the key conclusions
        - Be encouraging but honest
        - Be exactly 5-6 lines long
        """
        
        logger.debug("Sending voice feedback generation request to OpenAI")
        voice_response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[{"role": "user", "content": voice_feedback_prompt}],
            temperature=0.5,
            max_tokens=300
        )
        
        voice_feedback = voice_response.choices[0].message.content
        logger.debug(f"Generated voice feedback: {voice_feedback}")
        
        logger.debug("Converting voice feedback to audio")
        voice_audio = text_to_speech(voice_feedback)
        
        return {
            "status": "success",
            "report": report_content,
            "voice_feedback": voice_feedback,
            "voice_audio": voice_audio,
            "status_class": status_class,
            "avg_rating": avg_rating,
            "duration": duration
        }
    except Exception as e:
        logger.error(f"Error generating report: {str(e)}", exc_info=True)
        return {
            "status": "error",
            "message": str(e),
            "report": "<p>Error generating report. Please try again.</p>",
            "voice_feedback": "We encountered an error generating your feedback.",
            "voice_audio": None
        }

class JSONEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, (datetime, np.integer)):
            return str(obj)
        return json.JSONEncoder.default(self, obj)

@app.route('/')
def home():
    logger.info("Home page accessed")
    session.clear()
    session['interview_data'] = init_interview_data()
    return render_template('index.html')

@app.route('/start_interview', methods=['POST'])
def start_interview():
    logger.info("Interview start request received")
    data = request.get_json()
    session['interview_data'] = init_interview_data()
    interview_data = session['interview_data']
    
    interview_data['role'] = data.get('role', 'Software Engineer')
    interview_data['experience_level'] = data.get('experience_level', 'fresher')
    interview_data['years_experience'] = int(data.get('years_experience', 0))
    interview_data['start_time'] = datetime.now(timezone.utc)
    interview_data['last_activity_time'] = datetime.now(timezone.utc)
    logger.debug(f"Interview parameters set - Role: {interview_data['role']}, Experience: {interview_data['experience_level']}, Years: {interview_data['years_experience']}")
    
    try:
        questions, question_topics = generate_initial_questions(
            interview_data['role'],
            interview_data['experience_level'],
            interview_data['years_experience']
        )
        
        interview_data['questions'] = [q["main"] for q in questions]
        interview_data['follow_up_questions'] = []
        interview_data['question_topics'] = question_topics
        
        for q in questions:
            interview_data['conversation_history'].append({
                "question": q["main"],
                "prepared_follow_ups": q["follow_ups"]
            })
        
        interview_data['interview_started'] = True
        session['interview_data'] = interview_data
        logger.info("Interview started successfully")
        
        return jsonify({
            "status": "started",
            "total_questions": len(interview_data['questions']),
            "welcome_message": f"Welcome to the interview for {interview_data['role']} position."
        })
    except Exception as e:
        logger.error(f"Error starting interview: {str(e)}", exc_info=True)
        return jsonify({"status": "error", "message": str(e)}), 500

@app.route('/get_question', methods=['GET'])
def get_question():
    logger.debug("Get question request received")
    interview_data = session.get('interview_data', init_interview_data())
    
    if not interview_data['interview_started']:
        logger.warning("Attempt to get question before interview started")
        return jsonify({"status": "not_started"})
    
    # First try to get a follow-up question if available and conditions are met
    if (interview_data['follow_up_questions'] and 
        interview_data['follow_up_count'] < MAX_FOLLOW_UPS and
        (interview_data['follow_up_count'] < MIN_FOLLOW_UPS or 
         np.random.random() < FOLLOW_UP_PROBABILITY)):
        
        # Get the first follow-up that hasn't been used yet
        for follow_up in interview_data['follow_up_questions']:
            if follow_up not in interview_data['used_follow_ups']:
                current_q = follow_up
                interview_data['used_follow_ups'].append(current_q)
                is_follow_up = True
                interview_data['follow_up_count'] += 1
                logger.debug(f"Selected follow-up question: {current_q}")
                break
        else:
            current_q = None
    else:
        current_q = None
    
    # If no follow-up was selected, get the next main question
    if not current_q:
        if interview_data['current_question'] >= len(interview_data['questions']):
            logger.info("All questions exhausted, interview complete")
            return jsonify({"status": "completed"})
        
        current_q = interview_data['questions'][interview_data['current_question']]
        if current_q in interview_data['used_questions']:
            interview_data['current_question'] += 1
            return get_question()  # Recursively get next question
        
        interview_data['used_questions'].append(current_q)
        is_follow_up = False
        interview_data['follow_up_count'] = 0
        interview_data['current_topic'] = interview_data['question_topics'][interview_data['current_question']]
        interview_data['current_question'] += 1
        logger.debug(f"Selected main question: {current_q}")
    
    interview_data['conversation_history'].append({"speaker": "bot", "text": current_q})
    save_conversation_to_file([{"speaker": "bot", "text": current_q}])
    interview_data['last_activity_time'] = datetime.now(timezone.utc)
    session['interview_data'] = interview_data
    
    logger.debug("Converting question to speech")
    audio_data = text_to_speech(current_q)
    
    return jsonify({
        "status": "success",
        "question": current_q,
        "audio": audio_data,
        "question_number": interview_data['current_question'],
        "total_questions": len(interview_data['questions']),
        "is_follow_up": is_follow_up
    })

@app.route('/process_answer', methods=['POST'])
def process_answer():
    logger.info("Process answer request received")
    interview_data = session.get('interview_data', init_interview_data())
    
    if not interview_data['interview_started']:
        logger.warning("Attempt to process answer before interview started")
        return jsonify({"status": "error", "message": "Interview not started"}), 400
    
    data = request.get_json()
    answer = data.get('answer', '').strip()
    frame_data = data.get('frame', None)
    logger.debug(f"Received answer length: {len(answer)} characters")
    
    if not answer:
        logger.warning("Empty answer received")
        return jsonify({"status": "error", "message": "Empty answer"}), 400
    
    # Get the last question asked (either main or follow-up)
    current_question = interview_data['conversation_history'][-1]['text']
    
    interview_data['answers'].append(answer)
    interview_data['conversation_history'].append({"speaker": "user", "text": answer})
    save_conversation_to_file([{"speaker": "user", "text": answer}])
    interview_data['last_activity_time'] = datetime.now(timezone.utc)
    
    visual_feedback = None
    current_time = datetime.now().timestamp()
    if frame_data and (current_time - interview_data['last_frame_time']) > FRAME_CAPTURE_INTERVAL:
        try:
            logger.debug("Processing frame data")
            frame_bytes = base64.b64decode(frame_data.split(',')[1])
            frame_array = np.frombuffer(frame_bytes, dtype=np.uint8)
            frame = cv2.imdecode(frame_array, cv2.IMREAD_COLOR)
            
            if frame is not None:
                frame_base64 = process_frame_for_gpt4v(frame)
                visual_feedback = analyze_visual_response(
                    frame_base64,
                    interview_data['conversation_history'][-3:]
                )
                if visual_feedback:
                    interview_data['visual_feedback'].append(visual_feedback)
                    interview_data['last_frame_time'] = current_time
                    logger.debug("Visual feedback processed and stored")
        except Exception as e:
            logger.error(f"Error processing frame: {str(e)}", exc_info=True)
    
    logger.debug("Evaluating response quality")
    rating = evaluate_response(
        answer, 
        current_question, 
        interview_data['role'],
        interview_data['experience_level'],
        visual_feedback
    )
    interview_data['ratings'].append(rating)
    logger.debug(f"Response rated: {rating}/10")
    
    # Generate follow-up questions only if we haven't exceeded the max follow-ups
    if (interview_data['current_topic'] and len(answer.split()) > 15 and 
        interview_data['follow_up_count'] < MAX_FOLLOW_UPS):
        
        # Get prepared follow-ups for the current main question
        current_main_question_index = interview_data['current_question'] - 1
        if (current_main_question_index < len(interview_data['conversation_history']) and 
            'prepared_follow_ups' in interview_data['conversation_history'][current_main_question_index]):
            
            prepared_follow_ups = interview_data['conversation_history'][current_main_question_index]['prepared_follow_ups']
            # Add only unique follow-ups that haven't been used yet
            for follow_up in prepared_follow_ups:
                if follow_up not in interview_data['used_follow_ups'] and follow_up not in interview_data['follow_up_questions']:
                    interview_data['follow_up_questions'].append(follow_up)
                    logger.debug(f"Added prepared follow-up: {follow_up}")
        
        # Generate dynamic follow-up if we still need more
        if len(interview_data['follow_up_questions']) < MAX_FOLLOW_UPS:
            logger.debug("Generating dynamic follow-up question")
            dynamic_follow_up = generate_dynamic_follow_up(
                interview_data['conversation_history'],
                interview_data['current_topic']
            )
            if dynamic_follow_up and dynamic_follow_up not in interview_data['used_follow_ups'] and dynamic_follow_up not in interview_data['follow_up_questions']:
                interview_data['follow_up_questions'].append(dynamic_follow_up)
                logger.debug(f"Added dynamic follow-up: {dynamic_follow_up}")
    
    session['interview_data'] = interview_data
    
    return jsonify({
        "status": "answer_processed",
        "current_question": interview_data['current_question'],
        "total_questions": len(interview_data['questions']),
        "interview_complete": interview_data['current_question'] >= len(interview_data['questions']) and not interview_data['follow_up_questions'],
        "has_follow_up": len(interview_data['follow_up_questions']) > 0
    })

@app.route('/check_pause', methods=['GET'])
def check_pause():
    logger.debug("Check pause request received")
    interview_data = session.get('interview_data', init_interview_data())
    
    if not interview_data['interview_started']:
        logger.warning("Attempt to check pause before interview started")
        return jsonify({"status": "not_started"})
    
    current_time = datetime.now(timezone.utc)
    last_activity = interview_data['last_activity_time']
    seconds_since_activity = (current_time - last_activity).total_seconds() if last_activity else 0
    logger.debug(f"Seconds since last activity: {seconds_since_activity}")
    
    if seconds_since_activity > PAUSE_THRESHOLD:
        logger.info(f"Pause detected ({seconds_since_activity}s), generating encouragement")
        encouragement = generate_encouragement_prompt(interview_data['conversation_history'])
        audio_data = text_to_speech(encouragement)
        interview_data['last_activity_time'] = current_time
        session['interview_data'] = interview_data
        
        return jsonify({
            "status": "pause_detected",
            "prompt": encouragement,
            "audio": audio_data
        })
    
    return jsonify({"status": "active"})

@app.route('/generate_report', methods=['GET'])
def generate_report():
    logger.info("Generate report request received")
    interview_data = session.get('interview_data', init_interview_data())
    
    if not interview_data['interview_started']:
        logger.warning("Attempt to generate report before interview started")
        return jsonify({"status": "error", "message": "Interview not started"}), 400
    
    if not interview_data['end_time']:
        interview_data['end_time'] = datetime.now(timezone.utc)
        session['interview_data'] = interview_data
        logger.debug("Set end time for interview")
    
    report = generate_interview_report(interview_data)
    logger.info("Interview report generated")
    return jsonify(report)

@app.route('/reset_interview', methods=['POST'])
def reset_interview():
    logger.info("Interview reset request received")
    session.clear()
    session['interview_data'] = init_interview_data()
    return jsonify({"status": "success", "message": "Interview reset successfully"})

if __name__ == '__main__':
    app.json_encoder = JSONEncoder
    logger.info("Starting Flask application")
    app.run(debug=True) his is app py code and this is index html code <!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Interview Bot</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <style>
        body {
            background-color: #f8f9fa;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        }
        .header {
            background: linear-gradient(135deg,#E52437,#FF6372);
            color: white;
            padding: 2rem 0;
            margin-bottom: 2rem;
            border-radius: 0 0 10px 10px;
        }
        .interview-container {
            height: 70vh;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
        }
        .mic-button {
            width: 100px;
            height: 100px;
            border-radius: 50%;
            background-color: #E52437;
            color: white;
            border: none;
            font-size: 24px;
            cursor: pointer;
            display: flex;
            justify-content: center;
            align-items: center;
            box-shadow: 0 4px 8px rgba(0,0,0,0.2);
            transition: all 0.3s;
        }
        .mic-button:hover {
            transform: scale(1.05);
            box-shadow: 0 6px 12px rgba(0,0,0,0.3);
        }
        .mic-button.recording {
            animation: pulse 1.5s infinite;
            background-color: #dc3545;
        }
        @keyframes pulse {
            0% { transform: scale(1); }
            50% { transform: scale(1.1); }
            100% { transform: scale(1); }
        }
        .progress-container {
            width: 100%;
            max-width: 500px;
            margin: 20px auto;
        }
        .progress-text {
            text-align: center;
            margin-bottom: 10px;
            font-weight: bold;
        }
        .interview-complete-message {
            background-color: #d4edda;
            color: #155724;
            padding: 15px;
            border-radius: 8px;
            margin-top: 20px;
            text-align: center;
            display: none;
        }
        .report-modal {
            max-width: 800px;
        }
        .report-content {
            line-height: 1.6;
            font-size: 0.95rem;
        }
        .report-header {
            background: linear-gradient(135deg, #6e8efb, #a777e3);
            color: white;
        }
        .final-feedback {
            margin-top: 20px;
        }
        .setup-section .card {
            background: linear-gradient(270deg, #FEF3F4 20%);
            color: #000000;
        }
        .interview-section {
            display: none;
        }
        .interview-section .card {
            background: linear-gradient(270deg, #FEF3F4 0%, #FFFFFF 100%);
            color: #000000;
            height: 80vh;
        }
        .start-new-btn {
            position: fixed;
            bottom: 20px;
            left: 20px;
            z-index: 1000;
            display: none;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
            padding: 10px 20px;
            font-weight: bold;
            background-color: #E52437;
            border-color: #E52437;
            color: white;
        }
        .start-new-btn:hover {
            background-color: #C41E2E;
            border-color: #C41E2E;
            color: white;
        }
        .experience-fields {
            display: none;
        }
        .report-info {
            background-color: #f8f9fa;
            padding: 15px;
            border-radius: 8px;
            margin-bottom: 20px;
        }
        .camera-container {
            position: relative;
            width: 320px;
            height: 240px;
            margin: 20px auto;
            border: 2px solid #E52437;
            border-radius: 8px;
            overflow: hidden;
        }
        .camera-feed {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }
        .status-badge {
            font-size: 1.2rem;
            padding: 0.5rem 1rem;
            border-radius: 50px;
        }
        .status-selected {
            background-color: #28a745;
            color: white;
        }
        .status-onhold {
            background-color: #ffc107;
            color: black;
        }
        .status-rejected {
            background-color: #dc3545;
            color: white;
        }
        .strengths-section {
            background-color: #f0fff0;
            padding: 15px;
            border-radius: 8px;
            margin-top: 20px;
        }
        .improvements-section {
            background-color: #fff8f0;
            padding: 15px;
            border-radius: 8px;
            margin-top: 20px;
        }
        .strengths-section h4, .improvements-section h4 {
            border-bottom: 2px solid #ddd;
            padding-bottom: 8px;
            margin-bottom: 15px;
        }
        .strengths-section i {
            margin-right: 10px;
        }
        .improvements-section i {
            margin-right: 10px;
        }
        .rating-badge {
            font-size: 0.9rem;
            padding: 0.25rem 0.5rem;
            border-radius: 4px;
            background-color: #6c757d;
            color: white;
        }
        .rating-excellent {
            background-color: #28a745;
        }
        .rating-good {
            background-color: #17a2b8;
        }
        .rating-average {
            background-color: #ffc107;
            color: black;
        }
        .rating-poor {
            background-color: #dc3545;
        }
        .report-section {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 20px;
        }
        .report-section h4 {
            color: #495057;
            border-bottom: 2px solid #dee2e6;
            padding-bottom: 8px;
        }
        .strength-item {
            background-color: #e6f7e6;
            padding: 10px;
            border-radius: 5px;
            margin-bottom: 10px;
        }
        .improvement-item {
            background-color: #fff3e6;
            padding: 10px;
            border-radius: 5px;
            margin-bottom: 10px;
        }
        .final-decision {
            font-weight: bold;
            font-size: 1.1rem;
        }
        .decision-selected {
            color: #28a745;
        }
        .decision-onhold {
            color: #ffc107;
        }
        .decision-rejected {
            color: #dc3545;
        }
        .report-table {
            width: 100%;
            margin-bottom: 1rem;
            color: #212529;
            border-collapse: collapse;
        }
        .report-table th, .report-table td {
            padding: 0.75rem;
            vertical-align: top;
            border-top: 1px solid #dee2e6;
        }
        .report-table thead th {
            vertical-align: bottom;
            border-bottom: 2px solid #dee2e6;
            background-color: rgba(0,0,0,.03);
        }
        .report-table tbody + tbody {
            border-top: 2px solid #dee2e6;
        }
    </style>
</head>
<body>
    <div class="container-fluid">
        <div class="row header">
            <div class="col-12 text-center">
                <h1><i class="fas fa-robot"></i> Voice Interview Bot</h1>
                <p class="tagline">Practice your interview skills with our AI interviewer</p>
            </div>
        </div>

        <div class="row setup-section">
            <div class="col-md-6 offset-md-3">
                <div class="card">
                    <div class="card-header">
                        <h3>Interview Setup</h3>
                    </div>
                    <div class="card-body">
                        <div class="mb-3">
                            <label for="role" class="form-label">Job Role</label>
                            <select class="form-select" id="role" required>
                                <option value="Software Engineer">Software Engineer</option>
                                <option value="Data Scientist">Data Scientist</option>
                                <option value="Product Manager">Product Manager</option>
                                <option value="UX Designer">UX Designer</option>
                                <option value="Marketing Specialist">Marketing Specialist</option>
                                <option value="Financial Analyst">Financial Analyst</option>
                            </select>
                        </div>
                        <div class="mb-3">
                            <label class="form-label">Experience Level</label>
                            <div class="form-check">
                                <input class="form-check-input" type="radio" name="experienceLevel" id="fresher" value="fresher" checked>
                                <label class="form-check-label" for="fresher">
                                    Fresher
                                </label>
                            </div>
                            <div class="form-check">
                                <input class="form-check-input" type="radio" name="experienceLevel" id="experienced" value="experienced">
                                <label class="form-check-label" for="experienced">
                                    Experienced
                                </label>
                            </div>
                        </div>
                        <div class="mb-3 experience-fields" id="experienceFields">
                            <label for="yearsExperience" class="form-label">Years of Experience</label>
                            <input type="number" class="form-control" id="yearsExperience" min="1" max="30" value="3">
                        </div>
                        <div class="camera-container" id="cameraContainer">
                            <video id="cameraFeed" class="camera-feed" autoplay playsinline></video>
                        </div>
                        <button type="button" id="startInterviewBtn" class="btn btn-lg w-100" style="background-color: #E52437; color: white;">
                            <i class="fas fa-play"></i> Start Interview
                        </button>
                    </div>
                </div>
            </div>
        </div>

        <div class="row interview-section">
            <div class="col-md-8 offset-md-2">
                <div class="card">
                    <div class="card-header d-flex justify-content-between align-items-center">
                        <h3>Voice Interview</h3>
                        <div id="progressText">Question <span id="currentQuestionNum">1</span> of <span id="totalQuestions">5</span></div>
                    </div>
                    <div class="card-body">
                        <div class="interview-container">
                            <div class="camera-container">
                                <video id="activeCameraFeed" class="camera-feed" autoplay playsinline></video>
                            </div>
                            <button id="micButton" class="mic-button">
                                <i class="fas fa-microphone"></i>
                            </button>
                            <div class="progress-container">
                                <div class="progress-text" id="progressTextDisplay">0% Complete</div>
                                <div class="progress">
                                    <div id="progressBar" class="progress-bar bg-success" role="progressbar" style="width: 0%"></div>
                                </div>
                            </div>
                            <div id="interviewCompleteMessage" class="interview-complete-message">
                                Interview completed! Thank you for participating.
                                <button id="viewReportBtn" class="btn btn-outline-success btn-sm ms-3">
                                    <i class="fas fa-chart-bar"></i> View Performance Report
                                </button>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Start new interview button -->
        <button id="startNewInterviewBtn" class="start-new-btn">
            <i class="fas fa-redo"></i> Start New Interview
        </button>
    </div>

    <!-- Report Modal -->
    <div class="modal fade" id="reportModal" tabindex="-1" aria-labelledby="reportModalLabel" aria-hidden="true">
        <div class="modal-dialog modal-lg report-modal">
            <div class="modal-content">
                <div class="modal-header report-header">
                    <h5 class="modal-title" id="reportModalLabel">Interview Evaluation Report</h5>
                    <button type="button" class="btn-close btn-close-white" data-bs-dismiss="modal" aria-label="Close"></button>
                </div>
                <div class="modal-body">
                    <div class="report-content" id="reportContent">
                        Loading report...
                    </div>
                </div>
                <div class="modal-footer">
                    <button type="button" class="btn btn-secondary" data-bs-dismiss="modal">Close</button>
                    <button type="button" class="btn btn-primary" id="printReportBtn">
                        <i class="fas fa-print"></i> Print Report
                    </button>
                    <button type="button" class="btn btn-success" id="downloadReportBtn">
                        <i class="fas fa-download"></i> Download PDF
                    </button>
                </div>
            </div>
        </div>
    </div>

    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jspdf/2.5.1/jspdf.umd.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.4.1/html2canvas.min.js"></script>
    <script>
        $(document).ready(function() {
            let recognition;
            let currentQuestionAudio;
            let isRecording = false;
            let interviewInProgress = false;
            let autoStartRecording = true;
            let mediaStream = null;
            
            // Initialize UI elements
            $('#interviewCompleteMessage').hide();
            $('#startNewInterviewBtn').hide();
            
            // Show/hide experience fields based on selection
            $('input[name="experienceLevel"]').change(function() {
                if ($('#experienced').is(':checked')) {
                    $('#experienceFields').show();
                } else {
                    $('#experienceFields').hide();
                }
            });

            // Initialize camera
            async function initCamera() {
                try {
                    if (mediaStream) {
                        mediaStream.getTracks().forEach(track => track.stop());
                    }
                    
                    mediaStream = await navigator.mediaDevices.getUserMedia({ 
                        video: {
                            width: { ideal: 640 },
                            height: { ideal: 480 },
                            facingMode: "user"
                        },
                        audio: false
                    });
                    
                    const cameraFeed = document.getElementById('cameraFeed');
                    const activeCameraFeed = document.getElementById('activeCameraFeed');
                    
                    if (cameraFeed) cameraFeed.srcObject = mediaStream;
                    if (activeCameraFeed) activeCameraFeed.srcObject = mediaStream;
                    
                    return true;
                } catch (err) {
                    console.error("Error accessing camera:", err);
                    return false;
                }
            }
            
            // Start camera when page loads
            initCamera();
            
            // Check for browser support
            if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
                alert('Your browser does not support speech recognition. Please use Chrome or Edge.');
                $('#micButton').prop('disabled', true);
            } else {
                // Initialize speech recognition
                const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                recognition = new SpeechRecognition();
                recognition.continuous = false;
                recognition.interimResults = true;
                recognition.maxAlternatives = 1;
                recognition.lang = 'en-US';
                
                // Configure longer listening with pause handling
                recognition.onaudiostart = function() {
                    isRecording = true;
                    $('#micButton').addClass('recording');
                };
                
                recognition.onsoundend = function() {
                    // Don't stop immediately on sound end
                    setTimeout(function() {
                        if (isRecording) {
                            recognition.stop();
                        }
                    }, 4000); // 4 seconds of silence before stopping
                };
                
                recognition.onerror = function(event) {
                    console.error('Speech recognition error', event.error);
                    isRecording = false;
                    $('#micButton').removeClass('recording');
                };
                
                recognition.onend = function() {
                    isRecording = false;
                    $('#micButton').removeClass('recording');
                };
                
                recognition.onresult = function(event) {
                    let finalTranscript = '';
                    let interimTranscript = '';
                    
                    for (let i = event.resultIndex; i < event.results.length; i++) {
                        const transcript = event.results[i][0].transcript;
                        if (event.results[i].isFinal) {
                            finalTranscript += transcript;
                        } else {
                            interimTranscript += transcript;
                        }
                    }
                    
                    if (finalTranscript) {
                        processAnswer(finalTranscript);
                    }
                };
            }
            
            // Start interview
            $('#startInterviewBtn').click(async function() {
                const cameraInitialized = await initCamera();
                if (!cameraInitialized) {
                    alert('Could not access camera. Interview will continue without visual analysis.');
                }
                
                const role = $('#role').val();
                const experienceLevel = $('input[name="experienceLevel"]:checked').val();
                const yearsExperience = experienceLevel === 'experienced' ? $('#yearsExperience').val() : 0;
                
                $(this).html('<i class="fas fa-spinner fa-spin"></i> Starting...').prop('disabled', true);
                
                $.ajax({
                    url: '/start_interview',
                    type: 'POST',
                    contentType: 'application/json',
                    data: JSON.stringify({
                        role: role,
                        experience_level: experienceLevel,
                        years_experience: yearsExperience
                    }),
                    success: function(response) {
                        if (response.status === 'started') {
                            $('.setup-section').hide();
                            $('.interview-section').show();
                            $('#totalQuestions').text(response.total_questions);
                            interviewInProgress = true;
                            
                            // Start with first question
                            askQuestion();
                        }
                    },
                    error: function(xhr) {
                        alert('Error: ' + (xhr.responseJSON?.message || 'Failed to start interview'));
                        $('#startInterviewBtn').html('<i class="fas fa-play"></i> Start Interview').prop('disabled', false);
                    }
                });
            });
            
            // Ask the current question
            function askQuestion() {
                $.get('/get_question', function(response) {
                    if (response.status === 'completed') {
                        interviewComplete();
                    } else if (response.status === 'success') {
                        $('#currentQuestionNum').text(response.question_number);
                        
                        // Update progress
                        updateProgress(response.question_number - 1, response.total_questions);
                        
                        // Play the question audio
                        playAudio(response.audio, function() {
                            // Automatically start recording after question is played
                            if (autoStartRecording) {
                                startRecording();
                            }
                        });
                    }
                }).fail(function(xhr) {
                    alert('Error getting question: ' + (xhr.responseJSON?.message || 'Unknown error'));
                });
            }
            
            // Start recording automatically
            function startRecording() {
                if (!isRecording && interviewInProgress) {
                    try {
                        recognition.start();
                    } catch(e) {
                        console.error('Error starting recognition:', e);
                    }
                }
            }
            
            // Process user's answer
            function processAnswer(answer) {
                if (!interviewInProgress) return;
                
                // Capture frame for visual analysis
                const video = document.getElementById('activeCameraFeed');
                const canvas = document.createElement('canvas');
                canvas.width = video.videoWidth;
                canvas.height = video.videoHeight;
                const ctx = canvas.getContext('2d');
                ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
                const frameData = canvas.toDataURL('image/jpeg');
                
                $.ajax({
                    url: '/process_answer',
                    type: 'POST',
                    contentType: 'application/json',
                    data: JSON.stringify({
                        answer: answer,
                        frame: frameData
                    }),
                    success: function(response) {
                        if (response.status === 'answer_processed') {
                            // Update progress
                            updateProgress(response.current_question, response.total_questions);
                            $('#currentQuestionNum').text(response.current_question + 1);
                            
                            if (response.interview_complete) {
                                interviewComplete();
                            } else {
                                // Move directly to next question
                                setTimeout(askQuestion, 1000);
                            }
                        }
                    },
                    error: function(xhr) {
                        alert('Error processing answer: ' + (xhr.responseJSON?.message || 'Unknown error'));
                    }
                });
            }
            
            // Handle interview completion
            function interviewComplete() {
                interviewInProgress = false;
                if (mediaStream) {
                    mediaStream.getTracks().forEach(track => track.stop());
                }
                $('#interviewCompleteMessage').show();
                $('#startNewInterviewBtn').show();
            }
            
            // Play audio from base64 data
            function playAudio(base64Data, onEndedCallback) {
                if (!base64Data) {
                    if (onEndedCallback) onEndedCallback();
                    return;
                }
                const audio = new Audio('data:audio/wav;base64,' + base64Data);
                audio.play();
                if (onEndedCallback) {
                    audio.onended = onEndedCallback;
                }
            }
            
            // Update progress display
            function updateProgress(current, total) {
                const percent = Math.round((current / total) * 100);
                $('#progressBar').css('width', percent + '%');
                $('#progressTextDisplay').text(`${percent}% Complete`);
            }
            
            // Microphone button click handler
            $('#micButton').click(function() {
                if (!interviewInProgress) return;
                
                if (isRecording) {
                    recognition.stop();
                } else {
                    try {
                        recognition.start();
                    } catch(e) {
                        console.error('Error starting recognition:', e);
                    }
                }
            });
            
            // Start new interview
            $('#startNewInterviewBtn').click(function() {
                $.ajax({
                    url: '/reset_interview',
                    type: 'POST',
                    success: function() {
                        // Reset UI
                        $('.interview-section').hide();
                        $('.setup-section').show();
                        $('#startInterviewBtn').html('<i class="fas fa-play"></i> Start Interview').prop('disabled', false);
                        $('#interviewCompleteMessage').hide();
                        $('#startNewInterviewBtn').hide();
                        $('#progressBar').css('width', '0%');
                        $('#progressTextDisplay').text('0% Complete');
                        
                        // Reset camera
                        initCamera();
                    },
                    error: function(xhr) {
                        alert('Error: ' + (xhr.responseJSON?.message || 'Failed to reset interview'));
                    }
                });
            });
            
            // View report button
            $('#viewReportBtn').click(function() {
                showInterviewReport();
            });
            
            // Print report button
            $('#printReportBtn').click(function() {
                window.print();
            });
            
            // Download report as PDF
            $('#downloadReportBtn').click(function() {
                const { jsPDF } = window.jspdf;
                const element = document.getElementById('reportContent');
                
                html2canvas(element, {
                    scale: 2,
                    logging: true,
                    useCORS: true
                }).then(canvas => {
                    const imgData = canvas.toDataURL('image/png');
                    const pdf = new jsPDF('p', 'mm', 'a4');
                    const imgWidth = 210; // A4 width in mm
                    const pageHeight = 295; // A4 height in mm
                    const imgHeight = canvas.height * imgWidth / canvas.width;
                    let heightLeft = imgHeight;
                    let position = 0;
                    
                    pdf.addImage(imgData, 'PNG', 0, position, imgWidth, imgHeight);
                    heightLeft -= pageHeight;
                    
                    while (heightLeft >= 0) {
                        position = heightLeft - imgHeight;
                        pdf.addPage();
                        pdf.addImage(imgData, 'PNG', 0, position, imgWidth, imgHeight);
                        heightLeft -= pageHeight;
                    }
                    
                    pdf.save('interview_report.pdf');
                });
            });
            
            // Show interview report
            function showInterviewReport() {
                $.ajax({
                    url: '/generate_report',
                    type: 'GET',
                    success: function(response) {
                        if (response.status === "success") {
                            // Display the formatted report
                            $('#reportContent').html(response.report);
                            
                            // Add status badge
                            const statusBadge = `<div class="report-info mb-4">
                                <h4>Interview Summary</h4>
                                <div class="d-flex justify-content-between align-items-center">
                                    <span class="final-decision decision-${response.status_class}">
                                        ${response.status === "Selected" ? "✅ " : response.status === "On Hold" ? "🔄 " : "❌ "}
                                        ${response.status}
                                    </span>
                                    <span class="badge ${getRatingBadgeClass(response.avg_rating)}">
                                        Average Rating: ${response.avg_rating.toFixed(1)}/10
                                    </span>
                                    <span class="badge bg-secondary">
                                        Duration: ${response.duration}
                                    </span>
                                </div>
                            </div>`;
                            
                            $('#reportContent').prepend(statusBadge);
                            
                            // Play the voice feedback
                            if (response.voice_audio) {
                                playAudio(response.voice_audio);
                            }
                            
                            // Show the modal
                            var reportModal = new bootstrap.Modal(document.getElementById('reportModal'));
                            reportModal.show();
                        } else {
                            $('#reportContent').html(`
                                <div class="alert alert-danger">
                                    <h4>Error Generating Report</h4>
                                    <p>${response.message || 'Failed to generate report. Please try again.'}</p>
                                </div>
                            `);
                            var reportModal = new bootstrap.Modal(document.getElementById('reportModal'));
                            reportModal.show();
                        }
                    },
                    error: function(xhr) {
                        $('#reportContent').html(`
                            <div class="alert alert-danger">
                                <h4>Error Loading Report</h4>
                                <p>${xhr.responseJSON?.message || 'Failed to load report. Please try again.'}</p>
                            </div>
                        `);
                        var reportModal = new bootstrap.Modal(document.getElementById('reportModal'));
                        reportModal.show();
                    }
                });
            }
            
            function getRatingBadgeClass(rating) {
                if (rating >= 8) return 'rating-excellent';
                if (rating >= 6) return 'rating-good';
                if (rating >= 4) return 'rating-average';
                return 'rating-poor';
            }
        });
    </script>
</body>
</html> 